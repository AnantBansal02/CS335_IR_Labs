{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\anant\\AppData\\Roaming\\nltk_data...\n",
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import reuters\n",
    "import nltk\n",
    "nltk.download('reuters')\n",
    "!unzip /root/nltk_data/corpora/reuters.zip -d /root/nltk_data/corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import reuters\n",
    "len(reuters.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=['acq','corn','crude', 'earn', 'grain', 'interest', 'money-fx', 'ship', 'trade', 'wheat']\n",
    "document_id_list=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for classs in classes:\n",
    "  current_class_document_id_list=reuters.fileids(classs);\n",
    "  document_id_list.extend(current_class_document_id_list)\n",
    "distinct_document_id_list=list(set(document_id_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_map={'acq':0,'corn':1,'crude':2, 'earn':3, 'grain':4, 'interest':5, 'money-fx':6, 'ship':7, 'trade':8, 'wheat':9}\n",
    "processed_doc_ids=[]\n",
    "processed_label_ids=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(distinct_document_id_list)):\n",
    "  if len(reuters.categories(distinct_document_id_list[i]))==1:\n",
    "    processed_label_ids.append(category_map[reuters.categories(distinct_document_id_list[i])[0]])\n",
    "    processed_doc_ids.append(distinct_document_id_list[i])\n",
    "  else:\n",
    "    count=0\n",
    "    for category in reuters.categories(distinct_document_id_list[i]):\n",
    "      if category in classes:\n",
    "        count+=1\n",
    "        required_category=category\n",
    "    if count == 1:  \n",
    "      processed_doc_ids.append(distinct_document_id_list[i])\n",
    "      processed_label_ids.append(category_map[required_category])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_preprocessing(processed_doc_ids):\n",
    "  sum=0\n",
    "  for id in processed_doc_ids:\n",
    "    count=0\n",
    "    for category in reuters.categories(id):\n",
    "        if category in classes:\n",
    "          count+=1\n",
    "    if count>1:\n",
    "      print('Not working')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8201\n"
     ]
    }
   ],
   "source": [
    "verify_preprocessing(processed_doc_ids)\n",
    "print(len(processed_doc_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\anant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "documents=reuters.paras(fileids=processed_doc_ids)\n",
    "words_in_dataset=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(documents)):\n",
    "  words_in_current_document=documents[j]\n",
    "  for i in range(len(words_in_current_document)):\n",
    "    words_in_dataset.extend(words_in_current_document[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "def stem_tokens(tokens):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))  \n",
    "    return stemmed\n",
    "document_ids_terms = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100736\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for document_id in processed_doc_ids: \n",
    "  document_ids_terms[document_id]=[]\n",
    "  for para in reuters.paras(fileids=[document_id])[0]:\n",
    "    document_ids_terms[document_id].extend(stem_tokens(para))\n",
    "  i+=1\n",
    "sum=0\n",
    "for abc in document_ids_terms:\n",
    "  sum+=len(abc)\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8201\n",
      "8201\n",
      "8201\n"
     ]
    }
   ],
   "source": [
    "print(len(set(processed_doc_ids)))\n",
    "print(len(processed_doc_ids))\n",
    "print(len(processed_doc_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_matrix = TfidfVectorizer(stop_words='english', input='content')\n",
    "tfs = tfidf_matrix.fit_transform([\" \".join(l) for l in document_ids_terms.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8201, 19977)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "\n",
    "    def __init__(self, n_clusters,n_iter):\n",
    "        self.data = pd.DataFrame()\n",
    "        self.n_clusters = n_clusters\n",
    "        self.centroids = pd.DataFrame()\n",
    "        self.clusters = np.ndarray(1)\n",
    "        self.old_centroids = pd.DataFrame()\n",
    "        self.verbose = False\n",
    "        self.predictions = list()\n",
    "        self.n_iter=n_iter\n",
    "\n",
    "    def train(self, df, verbose):\n",
    "        self.verbose = verbose\n",
    "        self.data = df.copy(deep=True)\n",
    "        self.clusters = np.zeros(len(self.data))\n",
    "        if 'species' in self.data.columns:\n",
    "            self.data.drop('species', axis=1, inplace=True)\n",
    "\n",
    "        unique_rows = self.data.drop_duplicates()\n",
    "        unique_rows.reset_index(drop=True, inplace=True)\n",
    "        self.centroids = unique_rows.sample(n=self.n_clusters)\n",
    "        self.centroids.reset_index(drop=True, inplace=True)\n",
    "        if self.verbose:\n",
    "            print(\"\\nRandomly initiated centroids:\")\n",
    "            print(self.centroids)\n",
    "\n",
    "        self.old_centroids = pd.DataFrame(np.zeros(shape=(self.n_clusters, self.data.shape[1])),\n",
    "                                          columns=self.data.columns)\n",
    "        while not self.old_centroids.equals(self.centroids) and self.n_iter!=0:\n",
    "            if self.verbose:\n",
    "                time.sleep(3)\n",
    "\n",
    "            self.old_centroids = self.centroids.copy(deep=True)\n",
    "\n",
    "            for row_i in range(0, len(self.data)):\n",
    "                distances = list()\n",
    "                point = self.data.iloc[row_i]\n",
    "\n",
    "                for row_c in range(0, len(self.centroids)):\n",
    "                    centroid = self.centroids.iloc[row_c]\n",
    "                    distances.append(np.linalg.norm(point - centroid))\n",
    "\n",
    "                self.clusters[row_i] = np.argmin(distances)\n",
    "            for cls in range(0, self.n_clusters):\n",
    "\n",
    "                cls_idx = np.where(self.clusters == cls)[0]\n",
    "\n",
    "                if len(cls_idx) == 0:\n",
    "                    self.centroids.loc[cls] = self.old_centroids.loc[cls]\n",
    "                else:\n",
    "                    self.centroids.loc[cls] = self.data.iloc[cls_idx].mean()\n",
    "                    \n",
    "                if self.verbose:\n",
    "                    print(\"\\nRow indices belonging to cluster {}: [n={}]\".format(cls, len(cls_idx)))\n",
    "                    print(cls_idx)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(\"\\nOld centroids:\")\n",
    "                print(self.old_centroids)\n",
    "                print(\"New centroids:\")\n",
    "                print(self.centroids)\n",
    "            self.n_iter-=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_clusters = 10\n",
    "kmeans = KMeans(n_clusters=number_of_clusters,n_iter=100)\n",
    "kmeans.train(df=pd.DataFrame(tfs.toarray()), verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.linear_assignment_ import linear_assignment\n",
    "def _make_cost_m(cm):\n",
    "    s = np.max(cm)\n",
    "    return (- cm + s)\n",
    "def align_predicted_and_correct_cluster_labesl(correct_clusters,predicted_clusters):\n",
    "  cm=confusion_matrix(correct_clusters,predicted_clusters)\n",
    "  print(cm)\n",
    "  print(np.trace(cm) / np.sum(cm))\n",
    "  indexes = linear_assignment(_make_cost_m(cm))\n",
    "  js = [e[1] for e in sorted(indexes, key=lambda x: x[0])]\n",
    "  cm2 = cm[:, js]\n",
    "  print(cm2)\n",
    "  print(np.trace(cm2) / np.sum(cm2))\n",
    "  return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes=align_predicted_and_correct_cluster_labesl(processed_label_ids,kmeans.labels_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfnmatrix=np.array([[2239,    0,    6,    1,   45,   16,   12,    0,    0,    0],\n",
    " [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
    " [  66,    0,  380,    0,    5,    0,    6,    0,    1,    0],\n",
    " [ 165,  806,    4,  944,  865,   17,    2,  484,  132,  511],\n",
    " [  94,    0,    1,    0,    6,    0,   12,    0,    0,    0],\n",
    " [  16,   0,    1 ,   0,    1,  225,   41,   0,    6,    0],\n",
    " [  30,    0 ,   0,    0,    5,  103,  349,    0,   12,    0],\n",
    " [ 156,    0,    2,    0,    3,    0,    5,    0,    0,    0],\n",
    " [  19,    0,    3,    0,   21,    1,  311,    0,   70,    0],\n",
    " [   1,   0,    0 ,   0,    0,    0,    0,    0,    0,    0],])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sum=0\n",
    "column_sum=0\n",
    "for j in range(10):\n",
    "  row_sum+=cfnmatrix[0][j]\n",
    "print(row_sum)\n",
    "for j in range(10):\n",
    "  column_sum+=cfnmatrix[j][0]\n",
    "column_sum-=cfnmatrix[0][0]\n",
    "print(column_sum)\n",
    "cfnmatrix.sum()-row_sum-column_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TP_TN_FP_FN(confusion_matrix,n):\n",
    "  table=pd.DataFrame(columns=['Class','True Positive','True Negative','False Positive','False Negative'])\n",
    "  for i in range(10):\n",
    "    total_sum=np.sum(confusion_matrix)\n",
    "    true_positive=confusion_matrix[i][i]\n",
    "    false_postive=0\n",
    "    for j in range(10):\n",
    "      if j!=i:\n",
    "        false_postive+=confusion_matrix[j][i]\n",
    "    false_negative=0\n",
    "    for j in range(10):\n",
    "      if j!=i:\n",
    "        false_negative+=confusion_matrix[i][j]\n",
    "    row_sum=0\n",
    "    column_sum=0\n",
    "    for j in range(10):\n",
    "      row_sum+=confusion_matrix[i][j]\n",
    "    for j in range(10):\n",
    "      column_sum+=confusion_matrix[j][i]\n",
    "    column_sum-=confusion_matrix[i][i]\n",
    "    true_negative=total_sum-row_sum-column_sum\n",
    "    \n",
    "    table.loc[len(table.index)] = [i,true_positive,true_negative,false_postive,false_negative]\n",
    "  return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe=TP_TN_FP_FN(cfnmatrix,10)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation_dictionary={}\n",
    "for abc in indexes:\n",
    "  transformation_dictionary[abc[0]]=abc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_label_ids=[]\n",
    "for processed_label_id in processed_label_ids:\n",
    "  processed_label_id\n",
    "  aligned_label_ids.append(transformation_dictionary[processed_label_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = classification_report(aligned_label_ids,kmeans.labels_)\n",
    "print('Classification report : \\n',matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "def show_metrics(correct_clusters,predicted_clusters):\n",
    "    contingency_matrix = metrics.cluster.contingency_matrix(correct_clusters, predicted_clusters)\n",
    "    purity_score= np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix)\n",
    "    print('Purity Score',purity_score)\n",
    "    print('Normalized Mutual Information',normalized_mutual_info_score(correct_clusters,predicted_clusters))\n",
    "    print('Rand Index',adjusted_rand_score(correct_clusters,predicted_clusters))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
